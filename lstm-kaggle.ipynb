{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Num GPUs Available:  1\n",
      "Loaded all data!\n",
      "(12221694, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21452/186136727.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ffill\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bfill\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Asset_ID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   6391\u001b[0m         \u001b[0md\u001b[0m  \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6392\u001b[0m         \"\"\"\n\u001b[1;32m-> 6393\u001b[1;33m         return super().sort_index(\n\u001b[0m\u001b[0;32m   6394\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6395\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   4553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4554\u001b[0m         \u001b[0mbaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4555\u001b[1;33m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4557\u001b[0m         \u001b[1;31m# reconstruct axis if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         return self.reindex_indexer(\n\u001b[0m\u001b[0;32m    866\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[0;32m    678\u001b[0m             )\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             new_blocks = [\n\u001b[0m\u001b[0;32m    681\u001b[0m                 blk.take_nd(\n\u001b[0;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             new_blocks = [\n\u001b[1;32m--> 681\u001b[1;33m                 blk.take_nd(\n\u001b[0m\u001b[0;32m    682\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1142\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m   1145\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "DEVICE = \"TPU\" #or \"GPU\"\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "N_ASSETS = 14\n",
    "WINDOW_SIZE = 15\n",
    "BATCH_SIZE = 1024\n",
    "PCT_VALIDATION = 10 # last 10% of the data are used as validation set\n",
    "\n",
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except: print(\"failed to initialize TPU\")\n",
    "    else: DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\n",
    "if DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "import datatable as dt\n",
    "extra_data_files = {0: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin', 2: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-bitcoin-cash', 1: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-bitcoin', 3: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-cardano', 4: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-dogecoin', 5: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-eos-io', 6: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-ethereum', 7: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-ethereum-classic', 8: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-iota', 9: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-litecoin', 11: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-monero', 10: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-maker', 12: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-stellar', 13: 'g-research-crypto-forecasting/input/cryptocurrency-extra-data-tron'}\n",
    "\n",
    "# Uncomment to load the original csv [slower]\n",
    "# orig_df_train = pd.read_csv(data_path + 'train.csv') \n",
    "# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n",
    "# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n",
    "\n",
    "orig_df_train = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_train.jay').to_pandas()\n",
    "df_asset_details = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\n",
    "supp_df_train = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()\n",
    "assets_details = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\n",
    "asset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\n",
    "asset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n",
    "\n",
    "def load_training_data_for_asset(asset_id, load_jay = True):\n",
    "    dfs = []\n",
    "    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n",
    "    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n",
    "    \n",
    "    if load_jay:\n",
    "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n",
    "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n",
    "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n",
    "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n",
    "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n",
    "    else: \n",
    "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n",
    "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n",
    "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n",
    "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n",
    "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n",
    "    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n",
    "    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n",
    "    df = df.sort_values('date')\n",
    "    return df\n",
    "\n",
    "def load_data_for_all_assets():\n",
    "    dfs = []\n",
    "    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "LOAD_STRICT = True\n",
    "\n",
    "# WHICH YEARS TO INCLUDE? YES=1 NO=0\n",
    "INC2021 = 0\n",
    "INC2020 = 0\n",
    "INC2019 = 0\n",
    "INC2018 = 0\n",
    "INC2017 = 0\n",
    "INCCOMP = 1\n",
    "INCSUPP = 1\n",
    "\n",
    "train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\n",
    "if DEBUG: train = train[10000000:]\n",
    "\n",
    "test = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()\n",
    "sample_prediction_df = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_example_sample_submission.jay').to_pandas()\n",
    "assets = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\n",
    "assets_order = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\n",
    "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
    "print(\"Loaded all data!\")\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "        \n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
    "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
    "\n",
    "def get_features(df, row = False):\n",
    "    df_feat = df\n",
    "    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n",
    "    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n",
    "    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n",
    "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
    "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
    "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
    "    df_feat['trade'] = df_feat['Close'] - df_feat['Open']\n",
    "    df_feat['gtrade'] = df_feat['trade'] / df_feat['Count']\n",
    "    df_feat['shadow1'] = df_feat['trade'] / df_feat['Volume']\n",
    "    df_feat['shadow3'] = df_feat['upper_Shadow'] / df_feat['Volume']\n",
    "    df_feat['shadow5'] = df_feat['lower_Shadow'] / df_feat['Volume']\n",
    "    df_feat['diff1'] = df_feat['Volume'] - df_feat['Count']\n",
    "    df_feat['mean1'] = (df_feat['shadow5'] + df_feat['shadow3']) / 2\n",
    "    df_feat['mean2'] = (df_feat['shadow1'] + df_feat['Volume']) / 2\n",
    "    df_feat['mean3'] = (df_feat['trade'] + df_feat['gtrade']) / 2\n",
    "    df_feat['mean4'] = (df_feat['diff1'] + df_feat['upper_Shadow']) / 2\n",
    "    df_feat['mean5'] = (df_feat['diff1'] + df_feat['lower_Shadow']) / 2\n",
    "    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n",
    "    df_feat['UPS'] = df_feat['UPS']\n",
    "    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n",
    "    df_feat['LOS'] = df_feat['LOS']\n",
    "    df_feat['RNG'] = ((df_feat['High'] - df_feat['Low']) / df_feat['VWAP'])\n",
    "    df_feat['RNG'] = df_feat['RNG']\n",
    "    df_feat['MOV'] = ((df_feat['Close'] - df_feat['Open']) / df_feat['VWAP'])\n",
    "    df_feat['MOV'] = df_feat['MOV']\n",
    "    df_feat['CLS'] = ((df_feat['Close'] - df_feat['VWAP']) / df_feat['VWAP'])\n",
    "    df_feat['CLS'] = df_feat['CLS']\n",
    "    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n",
    "    df_feat['LOGVOL'] = df_feat['LOGVOL']\n",
    "    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n",
    "    df_feat['LOGCNT'] = df_feat['LOGCNT']\n",
    "    df_feat[\"Close/Open\"] = df_feat[\"Close\"] / df_feat[\"Open\"]\n",
    "    df_feat[\"Close-Open\"] = df_feat[\"Close\"] - df_feat[\"Open\"]\n",
    "    df_feat[\"High-Low\"] = df_feat[\"High\"] - df_feat[\"Low\"]\n",
    "    df_feat[\"High/Low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
    "    if row: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n",
    "    else: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis = 1)\n",
    "    df_feat[\"High/Mean\"] = df_feat[\"High\"] / df_feat[\"Mean\"]\n",
    "    df_feat[\"Low/Mean\"] = df_feat[\"Low\"] / df_feat[\"Mean\"]\n",
    "    df_feat[\"Volume/Count\"] = df_feat[\"Volume\"] / (df_feat[\"Count\"] + 1)\n",
    "    mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
    "    median_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
    "    df_feat['high2mean'] = df_feat['High'] / mean_price\n",
    "    df_feat['low2mean'] = df_feat['Low'] / mean_price\n",
    "    df_feat['high2median'] = df_feat['High'] / median_price\n",
    "    df_feat['low2median'] = df_feat['Low'] / median_price\n",
    "    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n",
    "    return df_feat\n",
    "\n",
    "train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\n",
    "print(train.shape)\n",
    "train['Target'] = train['Target'].fillna(0)\n",
    "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
    "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
    "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n",
    "df = train[['Asset_ID', 'Target']].copy()\n",
    "times = dict((t,i) for i,t in enumerate(df.index.unique()))\n",
    "df['id'] = df.index.map(times)\n",
    "df['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\n",
    "ids = df.id.copy()\n",
    "del df\n",
    "train = get_features(train)\n",
    "train_features = [i for i in train.columns if i not in ['Target', 'date', 'timestamp', 'Asset_ID', 'groups']]\n",
    "\n",
    "train = train.sort_index()\n",
    "ind = train.index.unique()\n",
    "def reindex(df):\n",
    "    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
    "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return df\n",
    "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
    "gc.collect()\n",
    "train.shape\n",
    "\n",
    "# Matching records and marking generated rows as 'non-real'\n",
    "train['group_num'] = train.index.map(times)\n",
    "train = train.dropna(subset=['group_num'])\n",
    "train['group_num'] = train['group_num'].astype('int')\n",
    "train['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n",
    "train['is_real'] = train.id.isin(ids) * 1\n",
    "train = train.drop('id', axis=1)\n",
    "\n",
    "# Features values for 'non-real' rows are set to zeros\n",
    "features = train.columns.drop(['Asset_ID','group_num','is_real'])\n",
    "train.loc[train.is_real == 0, features] = 0.\n",
    "\n",
    "train['asset_order'] = train.Asset_ID.map(assets_order)\n",
    "train = train.sort_values(by=['group_num', 'asset_order'])\n",
    "train = reduce_mem_usage(train)\n",
    "train.head(20)\n",
    "gc.collect()\n",
    "\n",
    "targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\n",
    "features = train.columns.drop(['Asset_ID', 'Target', 'group_num', 'is_real', 'date'])\n",
    "train = train[features]\n",
    "train = train.values\n",
    "train = train.reshape(-1, N_ASSETS, train.shape[-1])\n",
    "\n",
    "class sample_generator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, length):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.size = len(x_set)\n",
    "    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x=[]\n",
    "        batch_y=[]\n",
    "        for i in range(self.batch_size):\n",
    "            start_ind = self.batch_size*idx + i\n",
    "            end_ind = start_ind + self.length \n",
    "            if end_ind <= self.size:\n",
    "                batch_x.append(self.x[start_ind : end_ind])\n",
    "                batch_y.append(self.y[end_ind -1])\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "    \n",
    "\n",
    "X_train, X_test = train[:-len(train)//PCT_VALIDATION], train[-len(train)//PCT_VALIDATION:]\n",
    "y_train, y_test = targets[:-len(train)//PCT_VALIDATION], targets[-len(train)//PCT_VALIDATION:]\n",
    "\n",
    "train_generator = sample_generator(X_train, y_train, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\n",
    "val_generator = sample_generator(X_test, y_test, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\n",
    "print(f'Sample shape: {train_generator[0][0].shape}')\n",
    "print(f'Target shape: {train_generator[0][1].shape}')\n",
    "\n",
    "def MaxCorrelation(y_true,y_pred): return -tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n",
    "def Correlation(y_true,y_pred): return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n",
    "\n",
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n",
    "\n",
    "def masked_mae(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n",
    "\n",
    "def masked_cosine(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)\n",
    "\n",
    "def get_squence_model(x):\n",
    "    x = layers.LSTM(units=32, return_sequences=True)(x)\n",
    "    return x\n",
    "\n",
    "def get_model(n_assets = 14):\n",
    "    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n",
    "    branch_outputs = []\n",
    "    for i in range(n_assets):\n",
    "        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n",
    "        a = layers.Masking(mask_value = 0., )(a)\n",
    "        a = get_squence_model(a)\n",
    "        a = layers.GlobalAvgPool1D()(a)\n",
    "        branch_outputs.append(a)\n",
    "    x = layers.Concatenate()(branch_outputs)\n",
    "    x = layers.Dense(units = 128)(x)\n",
    "    out = layers.Dense(units = n_assets)(x)\n",
    "    model = keras.Model(inputs=x_input, outputs=out)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss = masked_cosine, metrics=[Correlation])\n",
    "    return model\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(get_model(n_assets=3), show_shapes=True)\n",
    "\n",
    "print(features)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "estop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 7, verbose = 0, mode = 'min',restore_best_weights = True)\n",
    "scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n",
    "lr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n",
    "history = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [lr, estop])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "histories = pd.DataFrame(history.history)\n",
    "epochs = list(range(1,len(histories)+1))\n",
    "loss = histories['loss']\n",
    "val_loss = histories['val_loss']\n",
    "Correlation = histories['Correlation']\n",
    "val_Correlation = histories['val_Correlation']\n",
    "ax[0].plot(epochs, loss, label = 'Train Loss')\n",
    "ax[0].plot(epochs, val_loss, label = 'Val Loss')\n",
    "ax[0].set_title('Losses')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[1].plot(epochs, Correlation, label = 'Train Correlation')\n",
    "ax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\n",
    "ax[1].set_title('Correlations')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(loc='upper right')\n",
    "fig.show()\n",
    "gc.collect()\n",
    "\n",
    "# The correlation coefficients by asset for the validation data\n",
    "predictions = model.predict(val_generator)\n",
    "\n",
    "print('Asset:    Corr. coef.')\n",
    "print('---------------------')\n",
    "for i in range(N_ASSETS):\n",
    "    # drop first 14 values in the y_test, since they are absent in val_generator labels\n",
    "    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n",
    "    y_pred = np.squeeze(predictions[:, i])\n",
    "    real_target_ind = np.argwhere(y_true!=0)\n",
    "    asset_id = list(assets_order.keys())[i]\n",
    "    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
    "    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\")\n",
    "    \n",
    "    \n",
    "sup = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()[:WINDOW_SIZE * (N_ASSETS)]\n",
    "placeholder = get_features(sup)\n",
    "placeholder['asset_order'] = placeholder.Asset_ID.map(assets_order)\n",
    "test_sample = np.array(placeholder[features])\n",
    "test_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\n",
    "test_sample = np.expand_dims(test_sample, axis=0)\n",
    "\n",
    "example = dt.fread('g-research-crypto-forecasting/input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()[:WINDOW_SIZE - 1]\n",
    "example['asset_order'] = example.Asset_ID.map(assets_order) \n",
    "example = example[['Asset_ID','asset_order']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "env = gresearch_crypto.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df = get_features(test_df)\n",
    "    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n",
    "    test = np.array(test_data[features].fillna(0))\n",
    "    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n",
    "    test_sample = np.hstack([test_sample, test])[:,-1 * WINDOW_SIZE:]\n",
    "    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n",
    "    test_data['Target'] = y_pred\n",
    "    for _, row in test_df.iterrows():\n",
    "        try: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n",
    "        except: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n",
    "    env.predict(sample_prediction_df)\n",
    "    \n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c993407023db842330a30138efed07ba84986807720160354e40b78ee1ec03e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
